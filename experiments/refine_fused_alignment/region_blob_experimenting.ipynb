{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import Image\n",
    "import os\n",
    "import sys\n",
    "os.chdir(r\"/home/yuval/Documents/XNOR/sealnet-mlflow\")\n",
    "\n",
    "import cv2\n",
    "from flow import s3_dataset, s3_cache\n",
    "from noaadb import Session\n",
    "from noaadb.schema.models import EOImage, HeaderMeta, IRLabelEntry, Species, IRImage, Flight\n",
    "from flow.util import extract_util\n",
    "\n",
    "red=[255,0,0]\n",
    "orange=[255,165,0]\n",
    "yellow=[255,255,0]\n",
    "green=[0,255,0]\n",
    "blue=[0,0,255]\n",
    "indigo=[75,0,130]\n",
    "violet=[238,130,238]\n",
    "COLORS = [red,orange,yellow,green,blue,indigo,violet]\n",
    "\n",
    "s = Session()\n",
    "\n",
    "\n",
    "ir_norm_local_path = s3_dataset.get_dataset_local_path('normalized_ir')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_im(im):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # load image using cv2....and do processing.\n",
    "    fig=plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.imshow(im,  cmap='gray', vmin=0, vmax=255)\n",
    "    # as opencv loads in BGR format by default, we want to show it in RGB.\n",
    "    plt.show()\n",
    "    \n",
    "# normalize IR\n",
    "def min_max_norm(im):\n",
    "    im_ir = ((im - np.min(im)) / (0.0 + np.max(im) - np.min(im)))\n",
    "    im_ir = im_ir*255.0\n",
    "    im_ir = im_ir.astype(np.uint8)\n",
    "#     print(im_ir.min(), im_ir.max())\n",
    "    return im_ir\n",
    "\n",
    "def standardize(im):\n",
    "    mu = im.mean()\n",
    "    std = im.std()\n",
    "    im = (im-mu)/std\n",
    "    im[im<0] = 0\n",
    "    im = im*255.0\n",
    "    return im\n",
    "\n",
    "def percentile(im):\n",
    "    mi = np.percentile( im, 1 )\n",
    "    ma = np.percentile( im, 100 )\n",
    "    normalized = ( im - mi ) / ( ma - mi )\n",
    "    normalized = normalized * 255\n",
    "    normalized[ normalized < 0 ] = 0\n",
    "    return normalized\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "im_obj = s.query(IRImage).filter(IRImage.file_name == 'polar_bear_2019_fl07_C_20190511_233032.965427_ir.tif').first()\n",
    "im_obj = s.query(IRImage).filter(IRImage.file_name == 'polar_bear_2019_fl07_C_20190511_225112.837455_ir.tif').first()\n",
    "im = cv2.imread(im_obj.file_path, cv2.IMREAD_ANYDEPTH)\n",
    "print(im.min(), im.max())\n",
    "\n",
    "im_norm = percentile(im)\n",
    "plot_im(im_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every cam, in each flight, in eaech survey sample n images that contain a label (images with hotspots)\n",
    "def sample_n_from_each_flight(n):\n",
    "    fl_cam_dict = {}\n",
    "    cams = extract_util.query_cfs(s,[],[],[])\n",
    "    for cam in cams:\n",
    "        if cam.cam_name == 'R':\n",
    "            continue\n",
    "        ir_images = s.query(IRImage).join(IRLabelEntry, IRLabelEntry.image_id == IRImage.file_name).join(HeaderMeta)\\\n",
    "            .filter(HeaderMeta.camera_id == cam.id).offset(100).limit(n).all()\n",
    "        print(cam.flight.flight_name)\n",
    "        fl_cam_dict[cam.flight.flight_name + '_' + cam.cam_name] = ir_images\n",
    "    return fl_cam_dict\n",
    "\n",
    "# draw ir boxes on a 1 channel image return 3 channel rgb\n",
    "def draw_ir_boxes(im_1c, im_row):\n",
    "    labels = s.query(IRLabelEntry).filter(IRLabelEntry.image_id == im_row.file_name).all()\n",
    "    color = cv2.cvtColor(im_1c,cv2.COLOR_GRAY2RGB)\n",
    "    for l in labels:\n",
    "        print(l.to_dict())\n",
    "        cv2.rectangle(color,(l.x1,l.y1),(l.x2,l.y2),(255,0,0),1)\n",
    "    return color, labels\n",
    "        \n",
    "def save_ir_with_labels(im_1c, im_row):\n",
    "    labels = s.query(IRLabelEntry).filter(IRLabelEntry.image_id == im_row.file_name).all()\n",
    "    color = cv2.cvtColor(im_1c,cv2.COLOR_GRAY2RGB)\n",
    "    for l in labels:\n",
    "        print(l.to_dict())\n",
    "        cv2.rectangle(color,(l.x1,l.y1),(l.x2,l.y2),(255,0,0),1)\n",
    "    file_uri = s3_dataset.get_dataset_uri('normalized_ir/%s' % im_row.file_name.replace(\".tif\", \".jpg\"))\n",
    "    s3_dataset.save_image_local(color, file_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Example IR Boxes experiment run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a run to show some examples of normalized IR imagery with boxes\n",
    "from flow import experiment, s3_cache, s3_dataset\n",
    "import mlflow\n",
    "\n",
    "IMS_PER_FLIGHT = 20\n",
    "with mlflow.start_run(run_name='example_ir_boxes', experiment_id=experiment.experiment_id) as mlrun:\n",
    "    mlflow.set_tag('method_fusion', True)\n",
    "    mlflow.set_tag('data-source', 'noaadb')\n",
    "    mlflow.set_tag('big_artifacts', True)\n",
    "\n",
    "    # log the function used for normalization\n",
    "    _, _ = s3_cache.log_function('normalization_function.txt', min_max_norm)\n",
    "\n",
    "\n",
    "    mlflow.log_param('data_uri', s3_dataset.get_dataset_uri('normalized_ir'))\n",
    "    mlflow.log_param('data_path', s3_dataset.get_dataset_local_path('normalized_ir'))\n",
    "\n",
    "    s=Session()\n",
    "\n",
    "    # get all IR images\n",
    "    ims_per_flight = sample_n_from_each_flight(IMS_PER_FLIGHT)\n",
    "    for k in ims_per_flight:\n",
    "        for im_row in ims_per_flight[k]:\n",
    "            im = cv2.imread(im_row.file_path, cv2.IMREAD_ANYDEPTH)\n",
    "            print(im.min(), im.max())\n",
    "            if im is None:\n",
    "                issue_image_list.append(image.file_name)\n",
    "                print('Issue:%s'%image.file_name)\n",
    "                continue\n",
    "            im_norm = min_max_norm(im)\n",
    "            ir_with_boxes, labels = draw_ir_boxes(im_norm, im_row)\n",
    "            \n",
    "            uri = os.path.join(mlflow.get_artifact_uri(), k, im_row.file_name.replace(\".tif\", \".png\"))\n",
    "            s3_cache.save_image_local(ir_with_boxes, uri)\n",
    "            mlflow.log_artifact(s3_cache.get_artifact_local_path(uri), k)\n",
    "            \n",
    "            label_list = [l.to_dict() for l in labels]\n",
    "            uri = os.path.join(mlflow.get_artifact_uri(), k, im_row.file_name.replace(\".tif\", \".txt\"))\n",
    "            s3_cache.save_list_local(label_list, uri)\n",
    "            mlflow.log_artifact(s3_cache.get_artifact_local_path(uri), k)\n",
    "            \n",
    "    if len(issue_image_list) > 0:\n",
    "        uri = mlflow.get_artifact_uri('could_not_load_images.txt')\n",
    "        local_path = s3_cache.save_list_local(issue_image_list, uri)\n",
    "        mlflow.log_artifact(local_path)\n",
    "    # end session\n",
    "    s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basic blob detecetor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a run to show some examples of normalized IR imagery with boxes\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def draw_keypoints(im, keypoints, color=(0,0,255)):\n",
    "    im_with_keypoints = cv2.drawKeypoints(im, keypoints, np.array([]), color, cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    return im_with_keypoints\n",
    "\n",
    "def draw_labels(im, labels, color=(255,0,0)):\n",
    "    for l in labels:\n",
    "        im = cv2.rectangle(im,(l.x1,l.y1),(l.x2,l.y2),color,1)\n",
    "    return im\n",
    "\n",
    "def dist(labels, keypoints):\n",
    "    n = len(labels)\n",
    "    m = len(keypoints)\n",
    "    dist_mat = np.zeros((n,m))\n",
    "    for j, keypoint in enumerate(keypoints):\n",
    "        k_cx = keypoint.pt[0]\n",
    "        k_cy = keypoint.pt[1]\n",
    "#         print(k_cx,k_cy)\n",
    "        for i,label in enumerate(labels):\n",
    "            l_cx = (label.x2 - label.x1)/2.0 + label.x1\n",
    "            l_cy = (label.y2 - label.y1)/2.0 + label.y1\n",
    "            dist = sqrt( (l_cx - k_cx)**2 + (l_cy - k_cy)**2)\n",
    "            dist_mat[i,j] = dist\n",
    "    \n",
    "    return dist_mat\n",
    "\n",
    "def dist_matches(dist_mat, labels, keypoints):\n",
    "#     min_idxs = dist_mat.argmin(axis=1)\n",
    "#     print(dist_mat[np.arange(len(min_idxs)), min_idxs])\n",
    "    min_dist_from_box = dist_mat.min(axis=1)\n",
    "    label_has_match = np.argwhere(min_dist_from_box<5).flatten()\n",
    "    label_match = np.array(labels)[label_has_match]\n",
    "    keypoint_idxs = dist_mat[label_has_match].argmin(axis=1)\n",
    "    keypoint_match = np.array(keypoints)[keypoint_idxs]\n",
    "    distances = dist_mat[label_has_match, keypoint_idxs]\n",
    "#     print(distances)\n",
    "    pairs = list(zip(keypoint_match, label_match, distances))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def detect(im):\n",
    "    params = cv2.SimpleBlobDetector_Params()\n",
    "#     params.minThreshold = 2\n",
    "    detector = cv2.SimpleBlobDetector_create(params)\n",
    "    keypoints = detector.detect(cv2.bitwise_not(im))\n",
    "#     print('%d keypoints' % len(keypoints))\n",
    "    return keypoints\n",
    "\n",
    "def draw_all_kpts(im_name = 'polar_bear_2019_fl06_C_20190510_224452.841718_ir.tif'):\n",
    "    # get image and labels\n",
    "    im_row = s.query(IRImage).filter(IRImage.file_name == im_name).first()\n",
    "    labels = s.query(IRLabelEntry).filter(IRLabelEntry.image_id == im_row.file_name).all()\n",
    "\n",
    "    # load image, then normalize\n",
    "    im = cv2.imread(im_row.file_path, cv2.IMREAD_ANYDEPTH)\n",
    "    im_norm = min_max_norm(im)\n",
    "    kpts = detect(im_norm)\n",
    "\n",
    "\n",
    "    color_im = cv2.cvtColor(im_norm,cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    color_im = draw_labels(color_im, labels)\n",
    "    color_im = draw_keypoints(color_im, kpts)\n",
    "    return kpts, labels, color_im\n",
    "\n",
    "def blob_detect_match(im_name = 'polar_bear_2019_fl06_C_20190510_224452.841718_ir.tif'):\n",
    "    # get image and labels\n",
    "    im_row = s.query(IRImage).filter(IRImage.file_name == im_name).first()\n",
    "    labels = s.query(IRLabelEntry).filter(IRLabelEntry.image_id == im_row.file_name).join(Species).filter(Species.name.contains('Seal')).all()\n",
    "    \n",
    "    # load image, then normalize\n",
    "    im = cv2.imread(im_row.file_path, cv2.IMREAD_ANYDEPTH)\n",
    "#     print(im_row.file_path)\n",
    "    \n",
    "    im_norm = min_max_norm(im)\n",
    "    kpts = detect(im_norm)\n",
    "#     print(kpts)\n",
    "\n",
    "    color_im = cv2.cvtColor(im_norm,cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    if len(kpts) == 0:\n",
    "        return color_im, []\n",
    "    \n",
    "    dists = dist(labels, kpts)\n",
    "    pairs = dist_matches(dists, labels, kpts)\n",
    "    for i,(k, l, d) in enumerate(pairs):\n",
    "        draw_color = COLORS[i%len(COLORS)]\n",
    "        cv2.putText(color_im,\"%.3f\"%d, (int(k.pt[0]), int(k.pt[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, draw_color, 1, cv2.LINE_AA)\n",
    "        color_im = draw_labels(color_im, [l], color=draw_color)\n",
    "        color_im = draw_keypoints(color_im, [k], color=draw_color)\n",
    "    return color_im, pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpts, labels, im = draw_all_kpts(im_name = 'polar_bear_2019_fl07_L_20190512_004950.615818_ir.tif')\n",
    "plot_im(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Run to sample and then visualize a few blobs/labels from each flight/cam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow import experiment, s3_cache, s3_dataset\n",
    "import mlflow\n",
    "# kpts, labels, im = draw_all_kpts(im_name = 'polar_bear_2019_fl07_L_20190512_004950.615818_ir.tif')\n",
    "# plot_im(im)\n",
    "IMS_PER_FLIGHT = 10\n",
    "with mlflow.start_run(run_name='example_blob_detect_match', experiment_id=experiment.experiment_id) as mlrun:\n",
    "    mlflow.set_tag('method_fusion', True)\n",
    "    mlflow.set_tag('data-source', 'noaadb')\n",
    "    mlflow.set_tag('big_artifacts', True)\n",
    "\n",
    "    # log the function used for normalization\n",
    "    _, _ = s3_cache.log_functions('code.txt', [min_max_norm,draw_all_kpts,blob_detect_match,detect,dist_matches, dist,draw_labels,draw_keypoints])\n",
    "\n",
    "\n",
    "    s=Session()\n",
    "\n",
    "    # get all IR images\n",
    "    ims_per_flight = sample_n_from_each_flight(IMS_PER_FLIGHT)\n",
    "    for k in ims_per_flight:\n",
    "        for im_row in ims_per_flight[k]:\n",
    "            # detect blobs\n",
    "            im, pairs = blob_detect_match(im_name = im_row.file_name)\n",
    "\n",
    "            uri = os.path.join(mlflow.get_artifact_uri(), k, im_row.file_name.replace(\".tif\", \"_matches.png\"))\n",
    "            s3_cache.save_image_local(im, uri)\n",
    "            mlflow.log_artifact(s3_cache.get_artifact_local_path(uri), k)\n",
    "            \n",
    "            # draw all\n",
    "            kpts, labels, im_all = draw_all_kpts(im_name = im_row.file_name)\n",
    "            uri_all = os.path.join(mlflow.get_artifact_uri(), k, im_row.file_name.replace(\".tif\", \"_all.jpg\"))\n",
    "            s3_cache.save_image_local(im_all, uri_all)\n",
    "            mlflow.log_artifact(s3_cache.get_artifact_local_path(uri_all), k)\n",
    "            \n",
    "            label_list = [l.to_dict() for l in labels]\n",
    "            uri = os.path.join(mlflow.get_artifact_uri(), k, im_row.file_name.replace(\".tif\", \".txt\"))\n",
    "            s3_cache.save_list_local(label_list, uri)\n",
    "            mlflow.log_artifact(s3_cache.get_artifact_local_path(uri), k)\n",
    "\n",
    "    # end session\n",
    "    s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mesure per flight distances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow import experiment, s3_cache, s3_dataset\n",
    "import mlflow\n",
    "def get_ir_for_all_flights():\n",
    "    cams = extract_util.query_cfs(s,[],[],[])\n",
    "    fl_cam_dict={}\n",
    "    for cam in cams:\n",
    "        if cam.cam_name == 'R':\n",
    "            continue\n",
    "        ir_images = s.query(IRImage).join(IRLabelEntry, IRLabelEntry.image_id == IRImage.file_name).join(Species).filter(Species.name.contains('Seal')).join(HeaderMeta)\\\n",
    "            .filter(HeaderMeta.camera_id == cam.id).all()\n",
    "        fl_cam_dict[cam.flight.flight_name + '_' + cam.cam_name] = ir_images\n",
    "    return fl_cam_dict\n",
    "\n",
    "def im_dist(file_name):\n",
    "    im_row = s.query(IRImage).filter(IRImage.file_name == file_name).first()\n",
    "    labels = s.query(IRLabelEntry).filter(IRLabelEntry.image_id == im_row.file_name).all()\n",
    "\n",
    "    # load image, then normalize\n",
    "    im = cv2.imread(im_row.file_path, cv2.IMREAD_ANYDEPTH)\n",
    "    im_norm = min_max_norm(im)\n",
    "    kpts = detect(im_norm)\n",
    "\n",
    "\n",
    "    if len(kpts) == 0:\n",
    "        return []\n",
    "    \n",
    "    dists = dist(labels, kpts)\n",
    "    pairs = dist_matches(dists, labels, kpts)\n",
    "    return pairs\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Session()\n",
    "\n",
    "with mlflow.start_run(run_name='measure_blob_distances', experiment_id=experiment.experiment_id) as mlrun:\n",
    "    mlflow.set_tag('data-source', 'noaadb')\n",
    "    _, _ = s3_cache.log_functions('code.txt', [min_max_norm,draw_all_kpts,blob_detect_match,detect,dist_matches, dist,im_dist])\n",
    "    fl_cam_dict = get_ir_for_all_flights()\n",
    "\n",
    "    distances = {}\n",
    "    for k in fl_cam_dict:\n",
    "        print('%s has %d images with labels' % (k, len(fl_cam_dict[k])))\n",
    "        if not k in distances:\n",
    "            distances[k] = []\n",
    "\n",
    "        for im in fl_cam_dict[k]:\n",
    "            pairs = im_dist(im.file_name)\n",
    "            for _, _, d in pairs:\n",
    "                distances[k].append(d)\n",
    "    for k in distances:\n",
    "        d = np.array(distances[k])\n",
    "        print('%s mean distance: %.2f, median: %.2f' % (k,d.mean(), np.median(d)))\n",
    "        mlflow.log_metric('%s_mean'%k, d.mean())\n",
    "        mlflow.log_metric('%s_median'%k, np.median(d))\n",
    "    \n",
    "s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Calc Distances by cam and flight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fl05_L has 405 images with labels\n",
      "fl05_C has 299 images with labels\n",
      "fl04_L has 555 images with labels\n",
      "fl04_C has 1043 images with labels\n",
      "fl07_L has 695 images with labels\n",
      "fl07_C has 557 images with labels\n",
      "fl06_L has 268 images with labels\n",
      "fl06_C has 206 images with labels\n",
      "fl05_L mean distance: 1.83, median: 1.63\n",
      "fl05_C mean distance: 1.59, median: 1.27\n",
      "fl04_L mean distance: 1.78, median: 1.49\n",
      "fl04_C mean distance: 1.31, median: 0.94\n",
      "fl07_L mean distance: 1.85, median: 1.54\n",
      "fl07_C mean distance: 1.82, median: 1.63\n",
      "fl06_L mean distance: 1.78, median: 1.65\n",
      "fl06_C mean distance: 1.03, median: 0.93\n"
     ]
    }
   ],
   "source": [
    "fl_cam_dict = get_ir_for_all_flights()\n",
    "\n",
    "distances = {}\n",
    "consecutive_missing = {}\n",
    "for k in fl_cam_dict:\n",
    "    consecutive_missing[k] = []\n",
    "    print('%s has %d images with labels' % (k, len(fl_cam_dict[k])))\n",
    "    if not k in distances:\n",
    "        distances[k] = []\n",
    "    consecutive = []\n",
    "    sorted_im_list = sorted(fl_cam_dict[k], key=lambda x: x.file_name, reverse=True)\n",
    "    for im in sorted_im_list: #fl_cam_dict[k]:\n",
    "        pairs = im_dist(im.file_name)\n",
    "        if len(pairs) == 0:\n",
    "            consecutive.append(im.file_name)\n",
    "        elif len(consecutive) > 0:\n",
    "            consecutive_missing[k].append(consecutive)\n",
    "            consecutive = []\n",
    "        for _, _, d in pairs:\n",
    "            distances[k].append(d)\n",
    "for k in distances:\n",
    "    d = np.array(distances[k])\n",
    "    print('%s mean distance: %.2f, median: %.2f' % (k,d.mean(), np.median(d)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Find consecutive groups of images where blob detector did not align well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "with mlflow.start_run(run_name='draw_consecutive_missing', experiment_id=experiment.experiment_id) as mlrun:\n",
    "    mlflow.set_tag('method_fusion', True)\n",
    "    mlflow.set_tag('data-source', 'noaadb')\n",
    "    mlflow.set_tag('big_artifacts', True)\n",
    "    \n",
    "    for fl_cam in consecutive_missing:\n",
    "        a = {}\n",
    "#         sorted_by_len =  sorted(consecutive_missing[fl_cam], key=lambda x:len(x), reverse=True)\n",
    "        for i, consecutive in enumerate(consecutive_missing[fl_cam]):\n",
    "            if len(consecutive) < 2:\n",
    "                continue\n",
    "            \n",
    "            group_key = '%0*d_consecutive_group' % (3, i)\n",
    "            a[group_key] = {'size': len(consecutive), 'images': []}\n",
    "            for im_name in consecutive:\n",
    "                kpts, labels, im_all = draw_all_kpts(im_name = im_name)\n",
    "                uri_all = os.path.join(mlflow.get_artifact_uri(), fl_cam, group_key, im_name.replace(\".tif\", \"_all.JPEG\"))\n",
    "                im_pil = Image.fromarray(im_all)\n",
    "                s3_cache.save_pil_image_local(im_pil, uri_all, quality=60)\n",
    "                mlflow.log_artifact(s3_cache.get_artifact_local_path(uri_all), os.path.join(fl_cam, group_key))\n",
    "                a[group_key]['images'].append(im_name)\n",
    "        \n",
    "#         l = a.items()\n",
    "        \n",
    "#         sorted_by_len =  sorted(l, key=lambda x:x[1]['size'], reverse=True)\n",
    "        out_lst = []\n",
    "        for k,x in l:\n",
    "            out_lst.append(k)\n",
    "            out_lst.append('length: %d' % x['size'])\n",
    "            for im in x['images']:\n",
    "                out_lst.append(im)\n",
    "            out_lst.append('')\n",
    "        uri = os.path.join(mlflow.get_artifact_uri(),fl_cam, 'consecutive_missing_groups.txt')\n",
    "        local_path = s3_cache.save_list_local(out_lst, uri)\n",
    "        mlflow.log_artifact(local_path, fl_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Session()\n",
    "labels = s.query(IRLabelEntry, Species).filter(IRLabelEntry.image_id == 'test_kotz_2019_fl04_C_20190510_012044.361914_ir.tif').join(Species).filter(Species.name.contains('Seal')).all()\n",
    "for label, sp in labels:\n",
    "    print(sp.name)\n",
    "s.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
